{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "log_dir = './logs/ML_Lab_10-04-NN_512x5-Xavier-Dropout'\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-a839aeb82f4b>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=[None, 784], dtype=tf.float32, name='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.placeholder(shape=[None, nb_classes], dtype=tf.int32, name='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(shape=None, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-7722a0ec5419>:5: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('layer1') as scope:\n",
    "    W1 = tf.get_variable('weight1', shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal(shape=[512]), shape=[512], dtype=tf.float32, name='bias1')\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "    tf.summary.histogram(\"W1\", W1)\n",
    "    tf.summary.histogram(\"b1\", b1)\n",
    "    tf.summary.histogram(\"Layer1\", L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer2') as scope:\n",
    "    W2 = tf.get_variable('weight2', shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal(shape=[512]), shape=[512], dtype=tf.float32, name='bias2')\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "    tf.summary.histogram(\"W2\", W2)\n",
    "    tf.summary.histogram(\"b2\", b2)\n",
    "    tf.summary.histogram(\"Layer2\", L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer3') as scope:\n",
    "    W3 = tf.get_variable('weight3', shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal(shape=[512]), shape=[512], dtype=tf.float32, name='bias3')\n",
    "    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "    L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "    tf.summary.histogram(\"W3\", W3)\n",
    "    tf.summary.histogram(\"b3\", b3)\n",
    "    tf.summary.histogram(\"Layer3\", L3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer4') as scope:\n",
    "    W4 = tf.get_variable('weight4', shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal(shape=[512]), shape=[512], dtype=tf.float32, name='bias4')\n",
    "    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "    L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "    tf.summary.histogram(\"W4\", W4)\n",
    "    tf.summary.histogram(\"b4\", b4)\n",
    "    tf.summary.histogram(\"Layer4\", L4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer5') as scope:\n",
    "    W5 = tf.get_variable('weight5', shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b5 = tf.Variable(tf.random_normal(shape=[10]), shape=[10], dtype=tf.float32, name='bias5')\n",
    "    hypothesis = tf.matmul(L4, W5) + b5\n",
    "    tf.summary.histogram(\"W5\", W5)\n",
    "    tf.summary.histogram(\"b5\", b5)\n",
    "    tf.summary.histogram(\"hypotheis\", hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('cost'):\n",
    "    cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=Y)\n",
    "    cost = tf.reduce_mean(cost_i)\n",
    "    tf.summary.scalar(\"Cost\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('Train'):\n",
    "    train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-154051bf3954>:2: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.math.argmax` instead\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('accuracy') as scope:\n",
    "    is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.485035747\n",
      "Accuracy:  0.9525\n",
      "Epoch: 0002 cost = 0.173654760\n",
      "Accuracy:  0.9685\n",
      "Epoch: 0003 cost = 0.128312111\n",
      "Accuracy:  0.973\n",
      "Epoch: 0004 cost = 0.108123342\n",
      "Accuracy:  0.9744\n",
      "Epoch: 0005 cost = 0.095715061\n",
      "Accuracy:  0.9762\n",
      "Epoch: 0006 cost = 0.083944001\n",
      "Accuracy:  0.9792\n",
      "Epoch: 0007 cost = 0.078073204\n",
      "Accuracy:  0.9793\n",
      "Epoch: 0008 cost = 0.069278378\n",
      "Accuracy:  0.9795\n",
      "Epoch: 0009 cost = 0.065217676\n",
      "Accuracy:  0.9769\n",
      "Epoch: 0010 cost = 0.062224372\n",
      "Accuracy:  0.9809\n",
      "Epoch: 0011 cost = 0.056993443\n",
      "Accuracy:  0.9789\n",
      "Epoch: 0012 cost = 0.054086716\n",
      "Accuracy:  0.9828\n",
      "Epoch: 0013 cost = 0.050521837\n",
      "Accuracy:  0.9825\n",
      "Epoch: 0014 cost = 0.049950259\n",
      "Accuracy:  0.9806\n",
      "Epoch: 0015 cost = 0.045259213\n",
      "Accuracy:  0.9804\n",
      "Epoch: 0016 cost = 0.042719241\n",
      "Accuracy:  0.9824\n",
      "Epoch: 0017 cost = 0.042972977\n",
      "Accuracy:  0.9823\n",
      "Epoch: 0018 cost = 0.040532109\n",
      "Accuracy:  0.9808\n",
      "Epoch: 0019 cost = 0.041096443\n",
      "Accuracy:  0.983\n",
      "Epoch: 0020 cost = 0.038150838\n",
      "Accuracy:  0.9828\n",
      "Epoch: 0021 cost = 0.035373995\n",
      "Accuracy:  0.9813\n",
      "Epoch: 0022 cost = 0.038577393\n",
      "Accuracy:  0.9815\n",
      "Epoch: 0023 cost = 0.035765467\n",
      "Accuracy:  0.9832\n",
      "Epoch: 0024 cost = 0.034827068\n",
      "Accuracy:  0.9808\n",
      "Epoch: 0025 cost = 0.034041381\n",
      "Accuracy:  0.9832\n",
      "Epoch: 0026 cost = 0.034506786\n",
      "Accuracy:  0.9833\n",
      "Epoch: 0027 cost = 0.034020635\n",
      "Accuracy:  0.9838\n",
      "Epoch: 0028 cost = 0.033192749\n",
      "Accuracy:  0.9853\n",
      "Epoch: 0029 cost = 0.030810861\n",
      "Accuracy:  0.9838\n",
      "Epoch: 0030 cost = 0.031455370\n",
      "Accuracy:  0.9837\n",
      "Epoch: 0031 cost = 0.029621737\n",
      "Accuracy:  0.9827\n",
      "Epoch: 0032 cost = 0.031613391\n",
      "Accuracy:  0.9825\n",
      "Epoch: 0033 cost = 0.033151038\n",
      "Accuracy:  0.9829\n",
      "Epoch: 0034 cost = 0.029401041\n",
      "Accuracy:  0.9819\n",
      "Epoch: 0035 cost = 0.029869538\n",
      "Accuracy:  0.9827\n",
      "Epoch: 0036 cost = 0.029972255\n",
      "Accuracy:  0.9843\n",
      "Epoch: 0037 cost = 0.031461326\n",
      "Accuracy:  0.9839\n",
      "Epoch: 0038 cost = 0.030715736\n",
      "Accuracy:  0.9835\n",
      "Epoch: 0039 cost = 0.027236207\n",
      "Accuracy:  0.9831\n",
      "Epoch: 0040 cost = 0.025391467\n",
      "Accuracy:  0.9829\n",
      "Epoch: 0041 cost = 0.031916473\n",
      "Accuracy:  0.9819\n",
      "Epoch: 0042 cost = 0.027895592\n",
      "Accuracy:  0.9825\n",
      "Epoch: 0043 cost = 0.023425077\n",
      "Accuracy:  0.984\n",
      "Epoch: 0044 cost = 0.027022171\n",
      "Accuracy:  0.9839\n",
      "Epoch: 0045 cost = 0.028931912\n",
      "Accuracy:  0.9827\n",
      "Epoch: 0046 cost = 0.029903798\n",
      "Accuracy:  0.9846\n",
      "Epoch: 0047 cost = 0.024735781\n",
      "Accuracy:  0.9821\n",
      "Epoch: 0048 cost = 0.028130670\n",
      "Accuracy:  0.9855\n",
      "Epoch: 0049 cost = 0.026968191\n",
      "Accuracy:  0.9842\n",
      "Epoch: 0050 cost = 0.025025664\n",
      "Accuracy:  0.9841\n",
      "Epoch: 0051 cost = 0.025349651\n",
      "Accuracy:  0.9838\n",
      "Epoch: 0052 cost = 0.028067293\n",
      "Accuracy:  0.9854\n",
      "Epoch: 0053 cost = 0.025750529\n",
      "Accuracy:  0.9837\n",
      "Epoch: 0054 cost = 0.025103709\n",
      "Accuracy:  0.984\n",
      "Epoch: 0055 cost = 0.029420007\n",
      "Accuracy:  0.9842\n",
      "Epoch: 0056 cost = 0.023351001\n",
      "Accuracy:  0.9847\n",
      "Epoch: 0057 cost = 0.025146805\n",
      "Accuracy:  0.9828\n",
      "Epoch: 0058 cost = 0.022425144\n",
      "Accuracy:  0.9843\n",
      "Epoch: 0059 cost = 0.023526903\n",
      "Accuracy:  0.9846\n",
      "Epoch: 0060 cost = 0.023660130\n",
      "Accuracy:  0.9833\n",
      "Epoch: 0061 cost = 0.028283289\n",
      "Accuracy:  0.9845\n",
      "Epoch: 0062 cost = 0.020696838\n",
      "Accuracy:  0.9844\n",
      "Epoch: 0063 cost = 0.023784024\n",
      "Accuracy:  0.9839\n",
      "Epoch: 0064 cost = 0.024330400\n",
      "Accuracy:  0.985\n",
      "Epoch: 0065 cost = 0.022160797\n",
      "Accuracy:  0.9844\n",
      "Epoch: 0066 cost = 0.025964455\n",
      "Accuracy:  0.9842\n",
      "Epoch: 0067 cost = 0.023445550\n",
      "Accuracy:  0.9831\n",
      "Epoch: 0068 cost = 0.023083946\n",
      "Accuracy:  0.9837\n",
      "Epoch: 0069 cost = 0.028679796\n",
      "Accuracy:  0.9843\n",
      "Epoch: 0070 cost = 0.022945988\n",
      "Accuracy:  0.9845\n",
      "Epoch: 0071 cost = 0.023384920\n",
      "Accuracy:  0.9828\n",
      "Epoch: 0072 cost = 0.024119213\n",
      "Accuracy:  0.9849\n",
      "Epoch: 0073 cost = 0.024323427\n",
      "Accuracy:  0.9826\n",
      "Epoch: 0074 cost = 0.024261356\n",
      "Accuracy:  0.9846\n",
      "Epoch: 0075 cost = 0.023460260\n",
      "Accuracy:  0.9829\n",
      "Epoch: 0076 cost = 0.023715323\n",
      "Accuracy:  0.9843\n",
      "Epoch: 0077 cost = 0.028328947\n",
      "Accuracy:  0.9827\n",
      "Epoch: 0078 cost = 0.024949673\n",
      "Accuracy:  0.9829\n",
      "Epoch: 0079 cost = 0.020451652\n",
      "Accuracy:  0.9845\n",
      "Epoch: 0080 cost = 0.021801185\n",
      "Accuracy:  0.9848\n",
      "Epoch: 0081 cost = 0.024281643\n",
      "Accuracy:  0.9836\n",
      "Epoch: 0082 cost = 0.024738069\n",
      "Accuracy:  0.9842\n",
      "Epoch: 0083 cost = 0.020475796\n",
      "Accuracy:  0.9843\n",
      "Epoch: 0084 cost = 0.025305245\n",
      "Accuracy:  0.9845\n",
      "Epoch: 0085 cost = 0.020027926\n",
      "Accuracy:  0.9806\n",
      "Epoch: 0086 cost = 0.022229255\n",
      "Accuracy:  0.9836\n",
      "Epoch: 0087 cost = 0.020126184\n",
      "Accuracy:  0.9822\n",
      "Epoch: 0088 cost = 0.019517524\n",
      "Accuracy:  0.9832\n",
      "Epoch: 0089 cost = 0.023358013\n",
      "Accuracy:  0.9842\n",
      "Epoch: 0090 cost = 0.019516105\n",
      "Accuracy:  0.9839\n",
      "Epoch: 0091 cost = 0.022486534\n",
      "Accuracy:  0.9852\n",
      "Epoch: 0092 cost = 0.022715431\n",
      "Accuracy:  0.9847\n",
      "Epoch: 0093 cost = 0.025588861\n",
      "Accuracy:  0.9828\n",
      "Epoch: 0094 cost = 0.022145659\n",
      "Accuracy:  0.983\n",
      "Epoch: 0095 cost = 0.023621921\n",
      "Accuracy:  0.9831\n",
      "Epoch: 0096 cost = 0.021825257\n",
      "Accuracy:  0.9841\n",
      "Epoch: 0097 cost = 0.026274038\n",
      "Accuracy:  0.985\n",
      "Epoch: 0098 cost = 0.020007833\n",
      "Accuracy:  0.9857\n",
      "Epoch: 0099 cost = 0.020977581\n",
      "Accuracy:  0.9847\n",
      "Epoch: 0100 cost = 0.021921718\n",
      "Accuracy:  0.9834\n",
      "Label: [4]\n",
      "Prediction: [4]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/Deep_NN\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(log_dir)\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, train], feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.7})\n",
    "            avg_cost += c / total_batch\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "        # Test he model using test sets\n",
    "        print('Accuracy: ', accuracy.eval(session=sess, feed_dict={X:mnist.test.images, Y: mnist.test.labels, keep_prob: 1.}))\n",
    "        \n",
    "        summary = sess.run(merged_summary, feed_dict={X:mnist.test.images, Y: mnist.test.labels, keep_prob: 1.})\n",
    "        writer.add_summary(summary, global_step=(epoch))\n",
    "        \n",
    "        #print('Accuracy: ', sess.run(accuracy, feed_dict={X:mnist.test.images, Y: mnist.test.labels}))\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples -1)\n",
    "    print('Label:', sess.run(tf.arg_max(mnist.test.labels[r:r+1], 1)))\n",
    "    print('Prediction:', sess.run(tf.arg_max(hypothesis, 1), feed_dict={X: mnist.test.images[r:r+1], keep_prob: 1.}))\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28,28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
