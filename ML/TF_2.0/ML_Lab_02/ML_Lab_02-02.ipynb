{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypothesis(object):\n",
    "    def __init__(self):\n",
    "        self.W = tf.Variable(tf.random.normal(shape=[1]), dtype=tf.float32, name='Weight')\n",
    "        self.b = tf.Variable(tf.random.normal(shape=[1]), dtype=tf.float32, name='bias')\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.W * x + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = Hypothesis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def loss(expected_y, observed_y):\n",
    "    return tf.reduce_mean(tf.square(expected_y - observed_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(model, inputs, outputs, learning_rate=0.01):\n",
    "    with tf.GradientTape() as t:\n",
    "        current_loss = loss(model(inputs), outputs)\n",
    "    dW, db = t.gradient(current_loss, [model.W, model.b])\n",
    "    model.W.assign_sub(learning_rate * dW)\n",
    "    model.b.assign_sub(learning_rate * db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   0: W=1.09 b=1.63, loss=0.66749\n",
      "step  20: W=0.82 b=1.48, loss=0.03770\n",
      "step  40: W=0.80 b=1.45, loss=0.02908\n",
      "step  60: W=0.81 b=1.43, loss=0.02637\n",
      "step  80: W=0.82 b=1.41, loss=0.02394\n",
      "step 100: W=0.83 b=1.39, loss=0.02175\n",
      "step 120: W=0.84 b=1.37, loss=0.01975\n",
      "step 140: W=0.84 b=1.35, loss=0.01794\n",
      "step 160: W=0.85 b=1.34, loss=0.01629\n",
      "step 180: W=0.86 b=1.32, loss=0.01480\n",
      "step 200: W=0.87 b=1.31, loss=0.01344\n",
      "step 220: W=0.87 b=1.29, loss=0.01220\n",
      "step 240: W=0.88 b=1.28, loss=0.01108\n",
      "step 260: W=0.88 b=1.26, loss=0.01007\n",
      "step 280: W=0.89 b=1.25, loss=0.00914\n",
      "step 300: W=0.89 b=1.24, loss=0.00830\n",
      "step 320: W=0.90 b=1.23, loss=0.00754\n",
      "step 340: W=0.90 b=1.22, loss=0.00685\n",
      "step 360: W=0.91 b=1.21, loss=0.00622\n",
      "step 380: W=0.91 b=1.20, loss=0.00565\n",
      "step 400: W=0.92 b=1.19, loss=0.00513\n",
      "step 420: W=0.92 b=1.18, loss=0.00466\n",
      "step 440: W=0.92 b=1.17, loss=0.00423\n",
      "step 460: W=0.93 b=1.16, loss=0.00384\n",
      "step 480: W=0.93 b=1.16, loss=0.00349\n",
      "step 500: W=0.93 b=1.15, loss=0.00317\n",
      "step 520: W=0.94 b=1.14, loss=0.00288\n",
      "step 540: W=0.94 b=1.14, loss=0.00262\n",
      "step 560: W=0.94 b=1.13, loss=0.00238\n",
      "step 580: W=0.95 b=1.12, loss=0.00216\n",
      "step 600: W=0.95 b=1.12, loss=0.00196\n",
      "step 620: W=0.95 b=1.11, loss=0.00178\n",
      "step 640: W=0.95 b=1.11, loss=0.00162\n",
      "step 660: W=0.96 b=1.10, loss=0.00147\n",
      "step 680: W=0.96 b=1.10, loss=0.00133\n",
      "step 700: W=0.96 b=1.09, loss=0.00121\n",
      "step 720: W=0.96 b=1.09, loss=0.00110\n",
      "step 740: W=0.96 b=1.08, loss=0.00100\n",
      "step 760: W=0.97 b=1.08, loss=0.00091\n",
      "step 780: W=0.97 b=1.08, loss=0.00082\n",
      "step 800: W=0.97 b=1.07, loss=0.00075\n",
      "step 820: W=0.97 b=1.07, loss=0.00068\n",
      "step 840: W=0.97 b=1.07, loss=0.00062\n",
      "step 860: W=0.97 b=1.06, loss=0.00056\n",
      "step 880: W=0.97 b=1.06, loss=0.00051\n",
      "step 900: W=0.98 b=1.06, loss=0.00046\n",
      "step 920: W=0.98 b=1.05, loss=0.00042\n",
      "step 940: W=0.98 b=1.05, loss=0.00038\n",
      "step 960: W=0.98 b=1.05, loss=0.00035\n",
      "step 980: W=0.98 b=1.05, loss=0.00031\n",
      "step 1000: W=0.98 b=1.04, loss=0.00029\n",
      "step 1020: W=0.98 b=1.04, loss=0.00026\n",
      "step 1040: W=0.98 b=1.04, loss=0.00024\n",
      "step 1060: W=0.98 b=1.04, loss=0.00021\n",
      "step 1080: W=0.98 b=1.04, loss=0.00019\n",
      "step 1100: W=0.98 b=1.04, loss=0.00018\n",
      "step 1120: W=0.99 b=1.03, loss=0.00016\n",
      "step 1140: W=0.99 b=1.03, loss=0.00015\n",
      "step 1160: W=0.99 b=1.03, loss=0.00013\n",
      "step 1180: W=0.99 b=1.03, loss=0.00012\n",
      "step 1200: W=0.99 b=1.03, loss=0.00011\n",
      "step 1220: W=0.99 b=1.03, loss=0.00010\n",
      "step 1240: W=0.99 b=1.03, loss=0.00009\n",
      "step 1260: W=0.99 b=1.02, loss=0.00008\n",
      "step 1280: W=0.99 b=1.02, loss=0.00007\n",
      "step 1300: W=0.99 b=1.02, loss=0.00007\n",
      "step 1320: W=0.99 b=1.02, loss=0.00006\n",
      "step 1340: W=0.99 b=1.02, loss=0.00006\n",
      "step 1360: W=0.99 b=1.02, loss=0.00005\n",
      "step 1380: W=0.99 b=1.02, loss=0.00005\n",
      "step 1400: W=0.99 b=1.02, loss=0.00004\n",
      "step 1420: W=0.99 b=1.02, loss=0.00004\n",
      "step 1440: W=0.99 b=1.02, loss=0.00003\n",
      "step 1460: W=0.99 b=1.01, loss=0.00003\n",
      "step 1480: W=0.99 b=1.01, loss=0.00003\n",
      "step 1500: W=0.99 b=1.01, loss=0.00003\n",
      "step 1520: W=0.99 b=1.01, loss=0.00002\n",
      "step 1540: W=0.99 b=1.01, loss=0.00002\n",
      "step 1560: W=0.99 b=1.01, loss=0.00002\n",
      "step 1580: W=1.00 b=1.01, loss=0.00002\n",
      "step 1600: W=1.00 b=1.01, loss=0.00002\n",
      "step 1620: W=1.00 b=1.01, loss=0.00001\n",
      "step 1640: W=1.00 b=1.01, loss=0.00001\n",
      "step 1660: W=1.00 b=1.01, loss=0.00001\n",
      "step 1680: W=1.00 b=1.01, loss=0.00001\n",
      "step 1700: W=1.00 b=1.01, loss=0.00001\n",
      "step 1720: W=1.00 b=1.01, loss=0.00001\n",
      "step 1740: W=1.00 b=1.01, loss=0.00001\n",
      "step 1760: W=1.00 b=1.01, loss=0.00001\n",
      "step 1780: W=1.00 b=1.01, loss=0.00001\n",
      "step 1800: W=1.00 b=1.01, loss=0.00001\n",
      "step 1820: W=1.00 b=1.01, loss=0.00001\n",
      "step 1840: W=1.00 b=1.01, loss=0.00001\n",
      "step 1860: W=1.00 b=1.01, loss=0.00000\n",
      "step 1880: W=1.00 b=1.01, loss=0.00000\n",
      "step 1900: W=1.00 b=1.01, loss=0.00000\n",
      "step 1920: W=1.00 b=1.00, loss=0.00000\n",
      "step 1940: W=1.00 b=1.00, loss=0.00000\n",
      "step 1960: W=1.00 b=1.00, loss=0.00000\n",
      "step 1980: W=1.00 b=1.00, loss=0.00000\n",
      "step 2000: W=1.00 b=1.00, loss=0.00000\n"
     ]
    }
   ],
   "source": [
    "Ws, bs = [], []\n",
    "for step in range(2001):\n",
    "    Ws.append(hypothesis.W.numpy())\n",
    "    bs.append(hypothesis.b.numpy())\n",
    "    current_loss = loss(hypothesis([1,2,3]), [2,3,4])\n",
    "    \n",
    "    train(hypothesis, [1,2,3], [2,3,4], learning_rate=0.01)\n",
    "    if step%20 == 0:\n",
    "        print('step %3d: W=%1.2f b=%1.2f, loss=%2.5f' % (step, Ws[-1], bs[-1], current_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
