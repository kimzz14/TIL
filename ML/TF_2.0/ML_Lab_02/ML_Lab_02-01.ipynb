{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.W = tf.Variable(tf.random.normal(shape=[1]), dtype=tf.float32, name='Weight')\n",
    "        self.b = tf.Variable(tf.random.normal(shape=[1]), dtype=tf.float32, name='bias')\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.W * x + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def loss(expected_y, observed_y):\n",
    "    return tf.reduce_mean(tf.square(expected_y - observed_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(model, inputs, outputs, learning_rate):\n",
    "    with tf.GradientTape() as t:\n",
    "        current_loss = loss(model(inputs), outputs)\n",
    "    dW, db = t.gradient(current_loss, [model.W, model.b])\n",
    "    model.W.assign_sub(learning_rate * dW)\n",
    "    model.b.assign_sub(learning_rate * db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   0: W=0.52 b=0.52, loss=1.06987\n",
      "step  20: W=0.89 b=0.89, loss=0.01386\n",
      "step  40: W=0.92 b=0.92, loss=0.00390\n",
      "step  60: W=0.93 b=0.93, loss=0.00346\n",
      "step  80: W=0.93 b=0.93, loss=0.00314\n",
      "step 100: W=0.94 b=0.94, loss=0.00286\n",
      "step 120: W=0.94 b=0.94, loss=0.00259\n",
      "step 140: W=0.94 b=0.94, loss=0.00236\n",
      "step 160: W=0.95 b=0.95, loss=0.00214\n",
      "step 180: W=0.95 b=0.95, loss=0.00194\n",
      "step 200: W=0.95 b=0.95, loss=0.00176\n",
      "step 220: W=0.95 b=0.95, loss=0.00160\n",
      "step 240: W=0.96 b=0.96, loss=0.00146\n",
      "step 260: W=0.96 b=0.96, loss=0.00132\n",
      "step 280: W=0.96 b=0.96, loss=0.00120\n",
      "step 300: W=0.96 b=0.96, loss=0.00109\n",
      "step 320: W=0.96 b=0.96, loss=0.00099\n",
      "step 340: W=0.97 b=0.97, loss=0.00090\n",
      "step 360: W=0.97 b=0.97, loss=0.00082\n",
      "step 380: W=0.97 b=0.97, loss=0.00074\n",
      "step 400: W=0.97 b=0.97, loss=0.00067\n",
      "step 420: W=0.97 b=0.97, loss=0.00061\n",
      "step 440: W=0.97 b=0.97, loss=0.00056\n",
      "step 460: W=0.97 b=0.97, loss=0.00050\n",
      "step 480: W=0.98 b=0.98, loss=0.00046\n",
      "step 500: W=0.98 b=0.98, loss=0.00042\n",
      "step 520: W=0.98 b=0.98, loss=0.00038\n",
      "step 540: W=0.98 b=0.98, loss=0.00034\n",
      "step 560: W=0.98 b=0.98, loss=0.00031\n",
      "step 580: W=0.98 b=0.98, loss=0.00028\n",
      "step 600: W=0.98 b=0.98, loss=0.00026\n",
      "step 620: W=0.98 b=0.98, loss=0.00023\n",
      "step 640: W=0.98 b=0.98, loss=0.00021\n",
      "step 660: W=0.98 b=0.98, loss=0.00019\n",
      "step 680: W=0.98 b=0.98, loss=0.00018\n",
      "step 700: W=0.99 b=0.99, loss=0.00016\n",
      "step 720: W=0.99 b=0.99, loss=0.00014\n",
      "step 740: W=0.99 b=0.99, loss=0.00013\n",
      "step 760: W=0.99 b=0.99, loss=0.00012\n",
      "step 780: W=0.99 b=0.99, loss=0.00011\n",
      "step 800: W=0.99 b=0.99, loss=0.00010\n",
      "step 820: W=0.99 b=0.99, loss=0.00009\n",
      "step 840: W=0.99 b=0.99, loss=0.00008\n",
      "step 860: W=0.99 b=0.99, loss=0.00007\n",
      "step 880: W=0.99 b=0.99, loss=0.00007\n",
      "step 900: W=0.99 b=0.99, loss=0.00006\n",
      "step 920: W=0.99 b=0.99, loss=0.00006\n",
      "step 940: W=0.99 b=0.99, loss=0.00005\n",
      "step 960: W=0.99 b=0.99, loss=0.00005\n",
      "step 980: W=0.99 b=0.99, loss=0.00004\n",
      "step 1000: W=0.99 b=0.99, loss=0.00004\n",
      "step 1020: W=0.99 b=0.99, loss=0.00003\n",
      "step 1040: W=0.99 b=0.99, loss=0.00003\n",
      "step 1060: W=0.99 b=0.99, loss=0.00003\n",
      "step 1080: W=0.99 b=0.99, loss=0.00003\n",
      "step 1100: W=0.99 b=0.99, loss=0.00002\n",
      "step 1120: W=0.99 b=0.99, loss=0.00002\n",
      "step 1140: W=0.99 b=0.99, loss=0.00002\n",
      "step 1160: W=1.00 b=1.00, loss=0.00002\n",
      "step 1180: W=1.00 b=1.00, loss=0.00002\n",
      "step 1200: W=1.00 b=1.00, loss=0.00001\n",
      "step 1220: W=1.00 b=1.00, loss=0.00001\n",
      "step 1240: W=1.00 b=1.00, loss=0.00001\n",
      "step 1260: W=1.00 b=1.00, loss=0.00001\n",
      "step 1280: W=1.00 b=1.00, loss=0.00001\n",
      "step 1300: W=1.00 b=1.00, loss=0.00001\n",
      "step 1320: W=1.00 b=1.00, loss=0.00001\n",
      "step 1340: W=1.00 b=1.00, loss=0.00001\n",
      "step 1360: W=1.00 b=1.00, loss=0.00001\n",
      "step 1380: W=1.00 b=1.00, loss=0.00001\n",
      "step 1400: W=1.00 b=1.00, loss=0.00001\n",
      "step 1420: W=1.00 b=1.00, loss=0.00000\n",
      "step 1440: W=1.00 b=1.00, loss=0.00000\n",
      "step 1460: W=1.00 b=1.00, loss=0.00000\n",
      "step 1480: W=1.00 b=1.00, loss=0.00000\n",
      "step 1500: W=1.00 b=1.00, loss=0.00000\n",
      "step 1520: W=1.00 b=1.00, loss=0.00000\n",
      "step 1540: W=1.00 b=1.00, loss=0.00000\n",
      "step 1560: W=1.00 b=1.00, loss=0.00000\n",
      "step 1580: W=1.00 b=1.00, loss=0.00000\n",
      "step 1600: W=1.00 b=1.00, loss=0.00000\n",
      "step 1620: W=1.00 b=1.00, loss=0.00000\n",
      "step 1640: W=1.00 b=1.00, loss=0.00000\n",
      "step 1660: W=1.00 b=1.00, loss=0.00000\n",
      "step 1680: W=1.00 b=1.00, loss=0.00000\n",
      "step 1700: W=1.00 b=1.00, loss=0.00000\n",
      "step 1720: W=1.00 b=1.00, loss=0.00000\n",
      "step 1740: W=1.00 b=1.00, loss=0.00000\n",
      "step 1760: W=1.00 b=1.00, loss=0.00000\n",
      "step 1780: W=1.00 b=1.00, loss=0.00000\n",
      "step 1800: W=1.00 b=1.00, loss=0.00000\n",
      "step 1820: W=1.00 b=1.00, loss=0.00000\n",
      "step 1840: W=1.00 b=1.00, loss=0.00000\n",
      "step 1860: W=1.00 b=1.00, loss=0.00000\n",
      "step 1880: W=1.00 b=1.00, loss=0.00000\n",
      "step 1900: W=1.00 b=1.00, loss=0.00000\n",
      "step 1920: W=1.00 b=1.00, loss=0.00000\n",
      "step 1940: W=1.00 b=1.00, loss=0.00000\n",
      "step 1960: W=1.00 b=1.00, loss=0.00000\n",
      "step 1980: W=1.00 b=1.00, loss=0.00000\n",
      "step 2000: W=1.00 b=1.00, loss=0.00000\n"
     ]
    }
   ],
   "source": [
    "Ws, bs = [], []\n",
    "for step in range(2001):\n",
    "    Ws.append(model.W.numpy())\n",
    "    bs.append(model.W.numpy())\n",
    "    current_loss = loss(model(inputs), outputs)\n",
    "    \n",
    "    train(model, inputs, outputs, learning_rate=0.01)\n",
    "    if step%20 == 0:\n",
    "        print('step %3d: W=%1.2f b=%1.2f, loss=%2.5f' % (step, Ws[-1], bs[-1], current_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
